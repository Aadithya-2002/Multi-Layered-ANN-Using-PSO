# -*- coding: utf-8 -*-
"""BIC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-jNNV1SrPpE7Yio6kz6POdtmGW4eI9y5
"""

import numpy as np
import random
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Activation functions
def relu(x):
    return np.maximum(0, x)

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def tanh(x):
    return np.tanh(x)

def linear(x):
    return x

# Xavier Initialization for Weights
def xavier_init(input_size, output_size):
    limit = np.sqrt(2 / (input_size + output_size))
    return np.random.uniform(-limit, limit, (input_size, output_size))

# Initialize ANN weights and biases
def initialize_ANN(layer_sizes):
    weights = []
    biases = []
    for i in range(len(layer_sizes) - 1):
        weights.append(xavier_init(layer_sizes[i], layer_sizes[i + 1]))
        biases.append(np.zeros((1, layer_sizes[i + 1])))
    return weights, biases

# Decode a PSO position into ANN weights and biases
def decode_position(position, layer_sizes):
    weights, biases, idx = [], [], 0
    for i in range(len(layer_sizes) - 1):
        weight_size = layer_sizes[i] * layer_sizes[i + 1]
        bias_size = layer_sizes[i + 1]
        weights.append(np.array(position[idx:idx + weight_size]).reshape(layer_sizes[i], layer_sizes[i + 1]))
        idx += weight_size
        biases.append(np.array(position[idx:idx + bias_size]).reshape(1, layer_sizes[i + 1]))
        idx += bias_size
    return weights, biases

# Feed forward through ANN
def feed_forward(X, weights, biases, activation_functions):
    for i in range(len(weights)):
        X = activation_functions[i](np.dot(X, weights[i]) + biases[i])
    return X

# PSO Optimization
def PSO(layer_sizes, activation_functions, X_train, y_train, X_val, y_val, swarm_size=300, iterations=300, alpha=0.7, beta=1.5, gamma=1.2):
    if len(layer_sizes) - 1 != len(activation_functions):
        raise ValueError("The number of activation functions must match the number of weight layers.")

    dimensions = sum(layer_sizes[i] * layer_sizes[i + 1] for i in range(len(layer_sizes) - 1)) + sum(layer_sizes[1:])
    swarm = [Particle(dimensions) for _ in range(swarm_size)]
    global_best_position = None
    global_best_fitness = float('inf')
    global_best_trend = []

    for iteration in range(iterations):
        for particle in swarm:
            weights, biases = decode_position(particle.position, layer_sizes)
            predictions = feed_forward(X_train, weights, biases, activation_functions)
            fitness = mean_squared_error(y_train, predictions)

            if fitness < particle.personal_best_fitness:
                particle.personal_best_position = particle.position[:]
                particle.personal_best_fitness = fitness

            if fitness < global_best_fitness:
                global_best_position = particle.position[:]
                global_best_fitness = fitness

        for particle in swarm:
            personal_best = np.array(particle.personal_best_position)
            global_best = np.array(global_best_position)
            position = np.array(particle.position)
            velocity = np.array(particle.velocity)

            particle.velocity = (
                alpha * velocity +
                beta * (personal_best - position) +
                gamma * (global_best - position)
            )
            particle.position = np.clip(position + particle.velocity, -30, 30)

        global_best_trend.append(global_best_fitness)
        print(f"Iteration {iteration}: Global Best Fitness (MSE): {global_best_fitness}")

        # Early stopping
        if len(global_best_trend) > 20 and np.abs(global_best_trend[-1] - global_best_trend[-20]) < 1e-6:
            print("Early stopping in PSO due to lack of improvement.")
            break

    return global_best_position, global_best_fitness, global_best_trend

# Main function
def main():
    np.random.seed(42)
    X = np.random.randn(1500, 8)
    y = np.random.randn(1500)

    # Normalize data using StandardScaler
    scaler = StandardScaler()
    X = scaler.fit_transform(X)
    y = (y - np.min(y)) / (np.max(y) - np.min(y))
    y = y.reshape(-1, 1)

    # Split into training and validation sets
    X_train, X_val = X[:1000], X[1000:]
    y_train, y_val = y[:1000], y[1000:]

    # ANN Configuration
    ann_configurations = [
        {"layers": [8, 30, 15, 1], "activations": [relu, tanh, linear]},
        {"layers": [8, 50, 30, 15, 1], "activations": [relu, tanh, sigmoid, linear]},
    ]

    # PSO Configurations
    pso_configurations = [
        {"swarm_size": 150, "iterations": 200, "alpha": 0.5, "beta": 1.2, "gamma": 1.0},
        {"swarm_size": 200, "iterations": 300, "alpha": 0.7, "beta": 1.5, "gamma": 1.2},
    ]

    results = []
    for ann_config in ann_configurations:
        for pso_config in pso_configurations:
            print(f"Running ANN Config: {ann_config['layers']} | PSO Config: {pso_config}")
            best_position, best_fitness, global_best_trend = PSO(
                ann_config["layers"],
                ann_config["activations"],
                X_train,
                y_train,
                X_val,
                y_val,
                swarm_size=pso_config["swarm_size"],
                iterations=pso_config["iterations"],
                alpha=pso_config["alpha"],
                beta=pso_config["beta"],
                gamma=pso_config["gamma"]
            )

            weights, biases = decode_position(best_position, ann_config["layers"])
            predictions = feed_forward(X_val, weights, biases, ann_config["activations"])
            mse = mean_squared_error(y_val, predictions)
            mae = mean_absolute_error(y_val, predictions)
            r2 = r2_score(y_val, predictions)

            results.append({
                "ANN Config": ann_config,
                "PSO Config": pso_config,
                "MSE": mse,
                "MAE": mae,
                "R2": r2,
                "Trend": global_best_trend
            })

    # Find the best configuration
    best_result = min(results, key=lambda x: x["MSE"])

    # Plot optimization trend for the best configuration
    plt.plot(best_result["Trend"])
    plt.title(f"Best PSO Optimization Trend (ANN: {best_result['ANN Config']['layers']})")
    plt.xlabel("Iteration")
    plt.ylabel("Global Best Fitness (MSE)")
    plt.show()

    # Plot predicted vs actual for the best configuration
    weights, biases = decode_position(best_position, best_result["ANN Config"]["layers"])
    predictions = feed_forward(X_val, weights, biases, best_result["ANN Config"]["activations"])

    plt.scatter(y_val, predictions)
    plt.title("Predicted vs Actual Values (Best Configuration)")
    plt.xlabel("Actual Values")
    plt.ylabel("Predicted Values")
    plt.show()

if __name__ == "__main__":
    main()